{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5e0ea3-9311-4264-9e57-ac5c08bcb952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/19 16:45:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"samir_mehdiyev_A3\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 8)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3715e17a-3646-4015-80bd-9fe5f6e33f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/02/19 16:45:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/02/19 16:45:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-215-de1:10005 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/02/19 16:45:22 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/19 16:45:23 INFO FileInputFormat: Total input files to process : 1\n",
      "24/02/19 16:45:23 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/02/19 16:45:23 INFO SparkContext: Starting job: count at /tmp/ipykernel_29018/3158762141.py:3\n",
      "24/02/19 16:45:23 INFO DAGScheduler: Got job 0 (count at /tmp/ipykernel_29018/3158762141.py:3) with 2 output partitions\n",
      "24/02/19 16:45:23 INFO DAGScheduler: Final stage: ResultStage 0 (count at /tmp/ipykernel_29018/3158762141.py:3)\n",
      "24/02/19 16:45:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:45:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:45:23 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_29018/3158762141.py:3), which has no missing parents\n",
      "24/02/19 16:45:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.2 KiB, free 434.1 MiB)\n",
      "24/02/19 16:45:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "24/02/19 16:45:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/19 16:45:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_29018/3158762141.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/19 16:45:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/02/19 16:45:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240219164521-0835/0 on worker-20240203224732-192.168.2.252-39355 (192.168.2.252:39355) with 8 core(s)\n",
      "24/02/19 16:45:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20240219164521-0835/0 on hostPort 192.168.2.252:39355 with 8 core(s), 1024.0 MiB RAM\n",
      "24/02/19 16:45:24 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/02/19 16:45:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240219164521-0835/0 is now RUNNING\n",
      "24/02/19 16:45:26 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.252:42778) with ID 0,  ResourceProfileId 0\n",
      "24/02/19 16:45:26 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "24/02/19 16:45:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.252:10006 with 434.4 MiB RAM, BlockManagerId(0, 192.168.2.252, 10006, None)\n",
      "24/02/19 16:45:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:45:26 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/19 16:45:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.252:10006 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/19 16:45:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.252:10006 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/02/19 16:45:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2411 ms on 192.168.2.252 (executor 0) (1/2)\n",
      "24/02/19 16:45:29 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2389 ms on 192.168.2.252 (executor 0) (2/2)\n",
      "24/02/19 16:45:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:29 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43317\n",
      "24/02/19 16:45:29 INFO DAGScheduler: ResultStage 0 (count at /tmp/ipykernel_29018/3158762141.py:3) finished in 5.498 s\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:45:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Job 0 finished: count at /tmp/ipykernel_29018/3158762141.py:3, took 5.622550 s\n",
      "24/02/19 16:45:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 221.5 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-215-de1:10005 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:29 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/19 16:45:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.252:10006 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/19 16:45:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on host-192-168-2-215-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:29 INFO FileInputFormat: Total input files to process : 1\n",
      "24/02/19 16:45:29 INFO SparkContext: Starting job: count at /tmp/ipykernel_29018/3158762141.py:8\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Got job 1 (count at /tmp/ipykernel_29018/3158762141.py:8) with 3 output partitions\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Final stage: ResultStage 1 (count at /tmp/ipykernel_29018/3158762141.py:8)\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_29018/3158762141.py:8), which has no missing parents\n",
      "24/02/19 16:45:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.2 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:29 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_29018/3158762141.py:8) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/19 16:45:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/02/19 16:45:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:45:29 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/19 16:45:29 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (192.168.2.252, executor 0, partition 2, ANY, 7690 bytes) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English line count: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.252:10006 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/19 16:45:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.252:10006 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:29 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 336 ms on 192.168.2.252 (executor 0) (1/3)\n",
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swedish line count: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1478 ms on 192.168.2.252 (executor 0) (2/3)\n",
      "24/02/19 16:45:30 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1573 ms on 192.168.2.252 (executor 0) (3/3)\n",
      "24/02/19 16:45:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:30 INFO DAGScheduler: ResultStage 1 (count at /tmp/ipykernel_29018/3158762141.py:8) finished in 1.587 s\n",
      "24/02/19 16:45:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:45:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/02/19 16:45:30 INFO DAGScheduler: Job 1 finished: count at /tmp/ipykernel_29018/3158762141.py:8, took 1.595876 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.1.1 Read the English transcripts with Spark, and count the number of lines.\n",
    "rdd_english = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en\")\n",
    "english_line_count = rdd_english.count()\n",
    "print(f\"English line count: {english_line_count}\")\n",
    "\n",
    "# A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each).\n",
    "rdd_swedish = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "swedish_line_count = rdd_swedish.count()\n",
    "print(f\"Swedish line count: {swedish_line_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5426be4c-0df8-4f61-b6c0-81dc12d980cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line counts match\n"
     ]
    }
   ],
   "source": [
    "#A.1.3 Verify that the line counts are the same for the two languages.\n",
    "if english_line_count == swedish_line_count:\n",
    "    print('Line counts match')\n",
    "else:\n",
    "    print('Line counts mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6b472e-f25e-4337-882d-7db1f5758bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English partitions: 2\n",
      "Swedish partitions: 3\n"
     ]
    }
   ],
   "source": [
    "# A.1.4 Count the number of partitions.\n",
    "english_partitions = rdd_english.getNumPartitions()\n",
    "swedish_partitions = rdd_swedish.getNumPartitions()\n",
    "\n",
    "print(f\"English partitions: {english_partitions}\")\n",
    "print(f\"Swedish partitions: {swedish_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd8572d-97af-4b31-8608-3e66b5eb0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.2.1 Pre-process the text from both RDDs\n",
    "def pre_process_text(line):\n",
    "    # Lowercase and tokenize\n",
    "    return line.lower().split(\" \")\n",
    "\n",
    "english_preprocessed = rdd_english.map(pre_process_text)\n",
    "swedish_preprocessed = rdd_swedish.map(pre_process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a33f948-1e7f-4454-9b07-10f9b2bddc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:31 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at PythonRDD.scala:181)\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/19 16:45:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.5 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:31 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/19 16:45:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/02/19 16:45:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:45:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.252:10006 (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 647 ms on 192.168.2.252 (executor 0) (1/1)\n",
      "24/02/19 16:45:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:31 INFO DAGScheduler: ResultStage 2 (runJob at PythonRDD.scala:181) finished in 0.671 s\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:45:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:181, took 0.680416 s\n",
      "24/02/19 16:45:31 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:181)\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/19 16:45:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.5 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/19 16:45:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/02/19 16:45:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:45:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.252:10006 (size: 5.2 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample: [['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.'], ['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.'], ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.'], ['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)'], ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.'], ['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.'], ['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swedish sample: [['återupptagande', 'av', 'sessionen'], ['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.'], ['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.'], ['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.'], ['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.'], ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.'], ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)'], ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.'], ['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.'], ['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 1180 ms on 192.168.2.252 (executor 0) (1/1)\n",
      "24/02/19 16:45:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:32 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:181) finished in 1.200 s\n",
      "24/02/19 16:45:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:45:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/02/19 16:45:32 INFO DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:181, took 1.208704 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing.\n",
    "print(\"English sample:\", english_preprocessed.take(10))\n",
    "print(\"Swedish sample:\", swedish_preprocessed.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b823cb3-96e6-41c9-bf6d-8e98b5e578ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:32 INFO SparkContext: Starting job: count at /tmp/ipykernel_29018/4224381202.py:2\n",
      "24/02/19 16:45:33 INFO DAGScheduler: Got job 4 (count at /tmp/ipykernel_29018/4224381202.py:2) with 2 output partitions\n",
      "24/02/19 16:45:33 INFO DAGScheduler: Final stage: ResultStage 4 (count at /tmp/ipykernel_29018/4224381202.py:2)\n",
      "24/02/19 16:45:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:45:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:45:33 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[8] at count at /tmp/ipykernel_29018/4224381202.py:2), which has no missing parents\n",
      "24/02/19 16:45:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.7 KiB, free 433.9 MiB)\n",
      "24/02/19 16:45:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:33 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (PythonRDD[8] at count at /tmp/ipykernel_29018/4224381202.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/19 16:45:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "24/02/19 16:45:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:45:33 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/19 16:45:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.252:10006 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:34 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 1974 ms on 192.168.2.252 (executor 0) (1/2)\n",
      "24/02/19 16:45:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 2003 ms on 192.168.2.252 (executor 0) (2/2)\n",
      "24/02/19 16:45:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:35 INFO DAGScheduler: ResultStage 4 (count at /tmp/ipykernel_29018/4224381202.py:2) finished in 2.020 s\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:45:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Job 4 finished: count at /tmp/ipykernel_29018/4224381202.py:2, took 2.027157 s\n",
      "24/02/19 16:45:35 INFO SparkContext: Starting job: count at /tmp/ipykernel_29018/4224381202.py:3\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Got job 5 (count at /tmp/ipykernel_29018/4224381202.py:3) with 3 output partitions\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Final stage: ResultStage 5 (count at /tmp/ipykernel_29018/4224381202.py:3)\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[9] at count at /tmp/ipykernel_29018/4224381202.py:3), which has no missing parents\n",
      "24/02/19 16:45:35 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.7 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:35 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:35 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (PythonRDD[9] at count at /tmp/ipykernel_29018/4224381202.py:3) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/19 16:45:35 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "24/02/19 16:45:35 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 9) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:45:35 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 10) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/19 16:45:35 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 11) (192.168.2.252, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/02/19 16:45:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.252:10006 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:35 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 11) in 440 ms on 192.168.2.252 (executor 0) (1/3)\n",
      "[Stage 5:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line counts match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:37 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 10) in 2766 ms on 192.168.2.252 (executor 0) (2/3)\n",
      "24/02/19 16:45:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 9) in 2850 ms on 192.168.2.252 (executor 0) (3/3)\n",
      "24/02/19 16:45:37 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:37 INFO DAGScheduler: ResultStage 5 (count at /tmp/ipykernel_29018/4224381202.py:3) finished in 2.872 s\n",
      "24/02/19 16:45:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:45:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/02/19 16:45:37 INFO DAGScheduler: Job 5 finished: count at /tmp/ipykernel_29018/4224381202.py:3, took 2.880994 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.2.3 Verify that the line counts still match after the pre-processing.\n",
    "english_preprocessed_line_count = english_preprocessed.count()\n",
    "swedish_preprocessed_line_count = swedish_preprocessed.count()\n",
    "\n",
    "if english_preprocessed_line_count == swedish_preprocessed_line_count:\n",
    "    print('Line counts match')\n",
    "else:\n",
    "    print('Line counts mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4326f2-9bb4-4973-8d9c-38e1988cca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:45:38 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_29018/1648545257.py:3\n",
      "24/02/19 16:45:38 INFO DAGScheduler: Registering RDD 11 (reduceByKey at /tmp/ipykernel_29018/1648545257.py:3) as input to shuffle 0\n",
      "24/02/19 16:45:38 INFO DAGScheduler: Got job 6 (takeOrdered at /tmp/ipykernel_29018/1648545257.py:3) with 2 output partitions\n",
      "24/02/19 16:45:38 INFO DAGScheduler: Final stage: ResultStage 7 (takeOrdered at /tmp/ipykernel_29018/1648545257.py:3)\n",
      "24/02/19 16:45:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "24/02/19 16:45:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
      "24/02/19 16:45:38 INFO DAGScheduler: Submitting ShuffleMapStage 6 (PairwiseRDD[11] at reduceByKey at /tmp/ipykernel_29018/1648545257.py:3), which has no missing parents\n",
      "24/02/19 16:45:38 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.2 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:38 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-215-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:38 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (PairwiseRDD[11] at reduceByKey at /tmp/ipykernel_29018/1648545257.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/19 16:45:38 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
      "24/02/19 16:45:38 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (192.168.2.252, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/02/19 16:45:38 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13) (192.168.2.252, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/02/19 16:45:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.252:10006 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 15222 ms on 192.168.2.252 (executor 0) (1/2)\n",
      "24/02/19 16:45:53 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 15841 ms on 192.168.2.252 (executor 0) (2/2)\n",
      "24/02/19 16:45:53 INFO DAGScheduler: ShuffleMapStage 6 (reduceByKey at /tmp/ipykernel_29018/1648545257.py:3) finished in 15.877 s\n",
      "24/02/19 16:45:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:53 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/19 16:45:53 INFO DAGScheduler: running: Set()\n",
      "24/02/19 16:45:53 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
      "24/02/19 16:45:53 INFO DAGScheduler: failed: Set()\n",
      "24/02/19 16:45:53 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[14] at takeOrdered at /tmp/ipykernel_29018/1648545257.py:3), which has no missing parents\n",
      "24/02/19 16:45:53 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.4 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:53 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:53 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on host-192-168-2-215-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:53 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (PythonRDD[14] at takeOrdered at /tmp/ipykernel_29018/1648545257.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/19 16:45:53 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
      "24/02/19 16:45:53 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14) (192.168.2.252, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:45:53 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 15) (192.168.2.252, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:45:53 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.252:10006 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.252:42778\n",
      "24/02/19 16:45:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 307 ms on 192.168.2.252 (executor 0) (1/2)\n",
      "24/02/19 16:45:54 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 15) in 313 ms on 192.168.2.252 (executor 0) (2/2)\n",
      "24/02/19 16:45:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:45:54 INFO DAGScheduler: ResultStage 7 (takeOrdered at /tmp/ipykernel_29018/1648545257.py:3) finished in 0.328 s\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:45:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Job 6 finished: takeOrdered at /tmp/ipykernel_29018/1648545257.py:3, took 16.271373 s\n",
      "24/02/19 16:45:54 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_29018/1648545257.py:3\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Registering RDD 16 (reduceByKey at /tmp/ipykernel_29018/1648545257.py:3) as input to shuffle 1\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Got job 7 (takeOrdered at /tmp/ipykernel_29018/1648545257.py:3) with 3 output partitions\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Final stage: ResultStage 9 (takeOrdered at /tmp/ipykernel_29018/1648545257.py:3)\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Submitting ShuffleMapStage 8 (PairwiseRDD[16] at reduceByKey at /tmp/ipykernel_29018/1648545257.py:3), which has no missing parents\n",
      "24/02/19 16:45:54 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.2 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:54 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.8 MiB)\n",
      "24/02/19 16:45:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on host-192-168-2-215-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:54 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:45:54 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 8 (PairwiseRDD[16] at reduceByKey at /tmp/ipykernel_29018/1648545257.py:3) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/19 16:45:54 INFO TaskSchedulerImpl: Adding task set 8.0 with 3 tasks resource profile 0\n",
      "24/02/19 16:45:54 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 16) (192.168.2.252, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/02/19 16:45:54 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 17) (192.168.2.252, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/02/19 16:45:54 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 18) (192.168.2.252, executor 0, partition 2, ANY, 7679 bytes) \n",
      "24/02/19 16:45:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.252:10006 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:45:56 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 18) in 2154 ms on 192.168.2.252 (executor 0) (1/3)\n",
      "24/02/19 16:46:09 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 17) in 14843 ms on 192.168.2.252 (executor 0) (2/3)\n",
      "24/02/19 16:46:09 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 16) in 15371 ms on 192.168.2.252 (executor 0) (3/3)\n",
      "24/02/19 16:46:09 INFO DAGScheduler: ShuffleMapStage 8 (reduceByKey at /tmp/ipykernel_29018/1648545257.py:3) finished in 15.386 s\n",
      "24/02/19 16:46:09 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/19 16:46:09 INFO DAGScheduler: running: Set()\n",
      "24/02/19 16:46:09 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "24/02/19 16:46:09 INFO DAGScheduler: failed: Set()\n",
      "24/02/19 16:46:09 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[19] at takeOrdered at /tmp/ipykernel_29018/1648545257.py:3), which has no missing parents\n",
      "24/02/19 16:46:09 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:46:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.4 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-215-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:46:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (PythonRDD[19] at takeOrdered at /tmp/ipykernel_29018/1648545257.py:3) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/19 16:46:09 INFO TaskSchedulerImpl: Adding task set 9.0 with 3 tasks resource profile 0\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.252:10006 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 19) (192.168.2.252, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:46:09 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 20) (192.168.2.252, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:46:09 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 21) (192.168.2.252, executor 0, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on host-192-168-2-215-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_7_piece0 on host-192-168-2-215-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.252:10006 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.252:10006 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.252:42778\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_8_piece0 on host-192-168-2-215-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.252:10006 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-215-de1:10005 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.252:10006 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on host-192-168-2-215-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.252:10006 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_9_piece0 on host-192-168-2-215-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.2.252:10006 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-215-de1:10005 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.252:10006 in memory (size: 5.2 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 English words: [('the', 3498375), ('of', 1659758), ('to', 1539760), ('and', 1288401), ('in', 1085993), ('that', 797516), ('a', 773522), ('is', 758050), ('for', 534242), ('we', 522849)]\n",
      "Top 10 Swedish words: [('att', 1706293), ('och', 1344830), ('i', 1050774), ('det', 924866), ('som', 913276), ('för', 908680), ('av', 738068), ('är', 694381), ('en', 620310), ('vi', 539797)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:46:10 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 20) in 285 ms on 192.168.2.252 (executor 0) (1/3)\n",
      "24/02/19 16:46:10 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 19) in 290 ms on 192.168.2.252 (executor 0) (2/3)\n",
      "24/02/19 16:46:10 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 21) in 313 ms on 192.168.2.252 (executor 0) (3/3)\n",
      "24/02/19 16:46:10 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:46:10 INFO DAGScheduler: ResultStage 9 (takeOrdered at /tmp/ipykernel_29018/1648545257.py:3) finished in 0.346 s\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:46:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Job 7 finished: takeOrdered at /tmp/ipykernel_29018/1648545257.py:3, took 15.746721 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language.\n",
    "def get_top_words(rdd):\n",
    "    return rdd.flatMap(lambda x: x).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "english_top_words = get_top_words(english_preprocessed)\n",
    "swedish_top_words = get_top_words(swedish_preprocessed)\n",
    "\n",
    "print(\"Top 10 English words:\", english_top_words)\n",
    "print(\"Top 10 Swedish words:\", swedish_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326ed6ab-927c-49e7-b46d-4ddee5b3bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.3.2 Verify that your results are reasonable.\n",
    "# Results look like correct, at least for english it is mostly correct that these are top frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294adb2b-8a65-4627-9919-654e66420347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:46:10 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_29018/584516410.py:6\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Got job 8 (zipWithIndex at /tmp/ipykernel_29018/584516410.py:6) with 2 output partitions\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Final stage: ResultStage 10 (zipWithIndex at /tmp/ipykernel_29018/584516410.py:6)\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[20] at zipWithIndex at /tmp/ipykernel_29018/584516410.py:6), which has no missing parents\n",
      "24/02/19 16:46:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.4 KiB, free 433.9 MiB)\n",
      "24/02/19 16:46:10 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.9 MiB)\n",
      "24/02/19 16:46:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:10 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:46:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (PythonRDD[20] at zipWithIndex at /tmp/ipykernel_29018/584516410.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/19 16:46:10 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
      "24/02/19 16:46:10 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:46:10 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/19 16:46:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.2.252:10006 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:12 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 2006 ms on 192.168.2.252 (executor 0) (1/2)\n",
      "24/02/19 16:46:12 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 2206 ms on 192.168.2.252 (executor 0) (2/2)\n",
      "24/02/19 16:46:12 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:46:12 INFO DAGScheduler: ResultStage 10 (zipWithIndex at /tmp/ipykernel_29018/584516410.py:6) finished in 2.221 s\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:46:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Job 8 finished: zipWithIndex at /tmp/ipykernel_29018/584516410.py:6, took 2.226774 s\n",
      "24/02/19 16:46:12 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_29018/584516410.py:7\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Got job 9 (zipWithIndex at /tmp/ipykernel_29018/584516410.py:7) with 3 output partitions\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Final stage: ResultStage 11 (zipWithIndex at /tmp/ipykernel_29018/584516410.py:7)\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[21] at zipWithIndex at /tmp/ipykernel_29018/584516410.py:7), which has no missing parents\n",
      "24/02/19 16:46:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 8.4 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on host-192-168-2-215-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:12 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:46:12 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (PythonRDD[21] at zipWithIndex at /tmp/ipykernel_29018/584516410.py:7) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/19 16:46:12 INFO TaskSchedulerImpl: Adding task set 11.0 with 3 tasks resource profile 0\n",
      "24/02/19 16:46:12 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 24) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/19 16:46:12 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 25) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/19 16:46:12 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 26) (192.168.2.252, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/02/19 16:46:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.252:10006 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:12 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 26) in 421 ms on 192.168.2.252 (executor 0) (1/3)\n",
      "24/02/19 16:46:15 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 25) in 2814 ms on 192.168.2.252 (executor 0) (2/3)\n",
      "24/02/19 16:46:15 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 24) in 2925 ms on 192.168.2.252 (executor 0) (3/3)\n",
      "24/02/19 16:46:15 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:46:15 INFO DAGScheduler: ResultStage 11 (zipWithIndex at /tmp/ipykernel_29018/584516410.py:7) finished in 2.936 s\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:46:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Job 9 finished: zipWithIndex at /tmp/ipykernel_29018/584516410.py:7, took 2.942110 s\n",
      "24/02/19 16:46:15 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_29018/584516410.py:34\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Registering RDD 26 (join at /tmp/ipykernel_29018/584516410.py:14) as input to shuffle 3\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Registering RDD 30 (reduceByKey at /tmp/ipykernel_29018/584516410.py:31) as input to shuffle 2\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Got job 10 (takeOrdered at /tmp/ipykernel_29018/584516410.py:34) with 5 output partitions\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Final stage: ResultStage 14 (takeOrdered at /tmp/ipykernel_29018/584516410.py:34)\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Submitting ShuffleMapStage 12 (PairwiseRDD[26] at join at /tmp/ipykernel_29018/584516410.py:14), which has no missing parents\n",
      "24/02/19 16:46:15 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.9 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:15 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on host-192-168-2-215-de1:10005 (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:15 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:46:15 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 12 (PairwiseRDD[26] at join at /tmp/ipykernel_29018/584516410.py:14) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/19 16:46:15 INFO TaskSchedulerImpl: Adding task set 12.0 with 5 tasks resource profile 0\n",
      "24/02/19 16:46:15 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 27) (192.168.2.252, executor 0, partition 0, ANY, 7788 bytes) \n",
      "24/02/19 16:46:15 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 28) (192.168.2.252, executor 0, partition 1, ANY, 7788 bytes) \n",
      "24/02/19 16:46:15 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 29) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/19 16:46:15 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 30) (192.168.2.252, executor 0, partition 3, ANY, 7788 bytes) \n",
      "24/02/19 16:46:15 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 31) (192.168.2.252, executor 0, partition 4, ANY, 7788 bytes) \n",
      "24/02/19 16:46:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.2.252:10006 (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:17 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 31) in 1858 ms on 192.168.2.252 (executor 0) (1/5)\n",
      "24/02/19 16:46:51 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 29) in 35703 ms on 192.168.2.252 (executor 0) (2/5)\n",
      "24/02/19 16:46:51 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 27) in 36019 ms on 192.168.2.252 (executor 0) (3/5)\n",
      "24/02/19 16:46:51 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 30) in 36293 ms on 192.168.2.252 (executor 0) (4/5)\n",
      "24/02/19 16:46:53 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 28) in 38114 ms on 192.168.2.252 (executor 0) (5/5)\n",
      "24/02/19 16:46:53 INFO DAGScheduler: ShuffleMapStage 12 (join at /tmp/ipykernel_29018/584516410.py:14) finished in 38.130 s\n",
      "24/02/19 16:46:53 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:46:53 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/19 16:46:53 INFO DAGScheduler: running: Set()\n",
      "24/02/19 16:46:53 INFO DAGScheduler: waiting: Set(ShuffleMapStage 13, ResultStage 14)\n",
      "24/02/19 16:46:53 INFO DAGScheduler: failed: Set()\n",
      "24/02/19 16:46:53 INFO DAGScheduler: Submitting ShuffleMapStage 13 (PairwiseRDD[30] at reduceByKey at /tmp/ipykernel_29018/584516410.py:31), which has no missing parents\n",
      "24/02/19 16:46:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.9 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.8 MiB)\n",
      "24/02/19 16:46:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on host-192-168-2-215-de1:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:53 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:46:53 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 13 (PairwiseRDD[30] at reduceByKey at /tmp/ipykernel_29018/584516410.py:31) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/19 16:46:53 INFO TaskSchedulerImpl: Adding task set 13.0 with 5 tasks resource profile 0\n",
      "24/02/19 16:46:53 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 32) (192.168.2.252, executor 0, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/02/19 16:46:53 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 33) (192.168.2.252, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/02/19 16:46:53 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 34) (192.168.2.252, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/02/19 16:46:53 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 35) (192.168.2.252, executor 0, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/02/19 16:46:53 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 36) (192.168.2.252, executor 0, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/02/19 16:46:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.252:10006 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:46:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.252:42778\n",
      "24/02/19 16:47:19 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 33) in 25893 ms on 192.168.2.252 (executor 0) (1/5)\n",
      "24/02/19 16:47:20 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 35) in 26389 ms on 192.168.2.252 (executor 0) (2/5)\n",
      "24/02/19 16:47:20 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 32) in 26703 ms on 192.168.2.252 (executor 0) (3/5)\n",
      "24/02/19 16:47:20 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 36) in 27151 ms on 192.168.2.252 (executor 0) (4/5)\n",
      "[Stage 13:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('is', 'är'), 4699), (('that', 'det'), 1494), (('we', 'vi'), 1443), (('the', 'jag'), 1336), (('is', 'debatten'), 1327), (('debate', 'förklarar'), 1319), (('the', 'debatten'), 1253), (('is', 'härmed'), 1240), (('debate', 'är'), 1211), (('i', 'jag'), 1168)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:47:21 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 34) in 27669 ms on 192.168.2.252 (executor 0) (5/5)\n",
      "24/02/19 16:47:21 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:47:21 INFO DAGScheduler: ShuffleMapStage 13 (reduceByKey at /tmp/ipykernel_29018/584516410.py:31) finished in 27.685 s\n",
      "24/02/19 16:47:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/19 16:47:21 INFO DAGScheduler: running: Set()\n",
      "24/02/19 16:47:21 INFO DAGScheduler: waiting: Set(ResultStage 14)\n",
      "24/02/19 16:47:21 INFO DAGScheduler: failed: Set()\n",
      "24/02/19 16:47:21 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[33] at takeOrdered at /tmp/ipykernel_29018/584516410.py:34), which has no missing parents\n",
      "24/02/19 16:47:21 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.3 KiB, free 433.8 MiB)\n",
      "24/02/19 16:47:21 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.8 MiB)\n",
      "24/02/19 16:47:21 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-215-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:47:21 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/19 16:47:21 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 14 (PythonRDD[33] at takeOrdered at /tmp/ipykernel_29018/584516410.py:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/19 16:47:21 INFO TaskSchedulerImpl: Adding task set 14.0 with 5 tasks resource profile 0\n",
      "24/02/19 16:47:21 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 37) (192.168.2.252, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:47:21 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 38) (192.168.2.252, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:47:21 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 39) (192.168.2.252, executor 0, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:47:21 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 40) (192.168.2.252, executor 0, partition 3, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:47:21 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 41) (192.168.2.252, executor 0, partition 4, NODE_LOCAL, 7437 bytes) \n",
      "24/02/19 16:47:21 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.252:10006 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/19 16:47:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.252:42778\n",
      "24/02/19 16:47:21 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 41) in 43 ms on 192.168.2.252 (executor 0) (1/5)\n",
      "24/02/19 16:47:21 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 40) in 45 ms on 192.168.2.252 (executor 0) (2/5)\n",
      "24/02/19 16:47:21 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 38) in 45 ms on 192.168.2.252 (executor 0) (3/5)\n",
      "24/02/19 16:47:21 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 39) in 48 ms on 192.168.2.252 (executor 0) (4/5)\n",
      "24/02/19 16:47:21 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 37) in 51 ms on 192.168.2.252 (executor 0) (5/5)\n",
      "24/02/19 16:47:21 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/02/19 16:47:21 INFO DAGScheduler: ResultStage 14 (takeOrdered at /tmp/ipykernel_29018/584516410.py:34) finished in 0.063 s\n",
      "24/02/19 16:47:21 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/19 16:47:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/02/19 16:47:21 INFO DAGScheduler: Job 10 finished: takeOrdered at /tmp/ipykernel_29018/584516410.py:34, took 65.914180 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two\n",
    "# languages. Do this by pairing words found on short lines with the same number of words\n",
    "# respectively. We (incorrectly) assume the words stay in the same order when translated.\n",
    "\n",
    "# Key the lines by their line number\n",
    "english_indexed = english_preprocessed.zipWithIndex()\n",
    "swedish_indexed = swedish_preprocessed.zipWithIndex()\n",
    "\n",
    "# Swap the key and value - so that the line number is the key.\n",
    "english_swapped = english_indexed.map(lambda x: (x[1], x[0]))\n",
    "swedish_swapped = swedish_indexed.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Join the two RDDs on the line number\n",
    "joined_rdd = english_swapped.join(swedish_swapped)\n",
    "\n",
    "# Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "filtered_non_empty = joined_rdd.filter(lambda x: x[1][0] and x[1][1])\n",
    "\n",
    "# Filter to leave only pairs of sentences with a small number of words per sentence\n",
    "filtered_short_sentences = filtered_non_empty.filter(lambda x: len(x[1][0]) <= 5 and len(x[1][1]) <= 5)\n",
    "\n",
    "# Filter to leave only pairs of sentences with the same number of words in each sentence.\n",
    "filtered_equal_words = filtered_short_sentences.filter(lambda x: len(x[1][0]) == len(x[1][1]))\n",
    "\n",
    "# For each sentence pair, map so that you pair each (in order) word in the two sentences. I add isalpha() check in order to get only words.\n",
    "word_pairs = word_pairs = filtered_equal_words.flatMap(\n",
    "    lambda x: [(en, sv) for en, sv in zip(x[1][0], x[1][1]) if en.isalpha() and sv.isalpha()]\n",
    ")\n",
    "\n",
    "# Use reduce to count the number of occurrences of the word-translation-pairs.\n",
    "word_pair_counts = word_pairs.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print some of the most frequently occurring pairs of words.\n",
    "most_frequent_pairs = word_pair_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "print(most_frequent_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9be158-77c5-4ba5-a097-7248242d84d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 16:55:35 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/02/19 16:55:35 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-215-de1:4040\n",
      "24/02/19 16:55:35 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/02/19 16:55:35 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/02/19 16:55:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/02/19 16:55:35 INFO MemoryStore: MemoryStore cleared\n",
      "24/02/19 16:55:35 INFO BlockManager: BlockManager stopped\n",
      "24/02/19 16:55:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/02/19 16:55:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/02/19 16:55:35 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367da88-4552-45a0-8ea7-93ba18226e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
